{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rocket DQN Training Notebook\n",
    "\n",
    "This notebook trains a Deep Q-Learning (DQN) neural network to control a rocket. \n",
    "The environment is defined in C++ (`rocket.cpp`) and exported as a shared library using **pybind11**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies and Compile Library\n",
    "\n",
    "We install necessary tools and compile the shared library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!apt-get update\n",
    "!apt-get install -y build-essential\n",
    "!pip install pybind11 torch matplotlib tqdm gymnasium\n",
    "!cd ..; make  # Compile the shared library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Shared Library\n",
    "\n",
    "Add the path where the shared library is located to `sys.path` to import it in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the root folder or 'lib' folder where the shared library is located\n",
    "sys.path.append(os.path.abspath('../lib'))  # Adjust according to your setup\n",
    "\n",
    "import Rocket as rck  # Import the Rocket class from the C++ library\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Define PyTorch device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Definition\n",
    "\n",
    "The `Red` class defines a fully connected neural network with two hidden layers and ReLU activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class Red(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_size=10, output_size=2):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory\n",
    "\n",
    "The `ReplayMemory` class stores transitions and generates random minibatches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "        self.deque_shuffle = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, transition):\n",
    "        \"\"\"Add a transition to the memory.\"\"\"\n",
    "        self.deque.append(transition)\n",
    "\n",
    "    def minibatch(self, batch_size=32):\n",
    "        \"\"\"Yield random minibatches for training.\"\"\"\n",
    "        if len(self.deque_shuffle) == 0:\n",
    "            self.deque_shuffle = copy.deepcopy(self.deque)\n",
    "            random.shuffle(self.deque_shuffle)\n",
    "        while len(self.deque_shuffle) > 0:\n",
    "            batch = []\n",
    "            for _ in range(min(batch_size, len(self.deque_shuffle))):\n",
    "                batch.append(self.deque_shuffle.popleft())\n",
    "            yield np.array(batch)\n",
    "\n",
    "    def reset(self, capacity=10000):\n",
    "        \"\"\"Reset memory to empty.\"\"\"\n",
    "        self.deque = deque(maxlen=capacity)\n",
    "        self.deque_shuffle = deque(maxlen=capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Definition\n",
    "\n",
    "The `WrapAgent` class implements an epsilon-greedy policy to choose actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class WrapAgent():\n",
    "    def __init__(self, epsilon=0.7, decay=10):\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        self.rng = np.random.default_rng(234343)\n",
    "\n",
    "    def choose_action(self, env, state, episode, Qnet, epsilon_o, step_epsilon, cont_epsilon):\n",
    "        \"\"\"Choose an action using epsilon-greedy policy.\"\"\"\n",
    "        rand = self.rng.uniform(0, 1)\n",
    "        if rand < (self.epsilon - step_epsilon * cont_epsilon) or rand < 0.01:\n",
    "            return env.sample()  # Random action\n",
    "        else:\n",
    "            with torch.inference_mode():\n",
    "                return torch.argmax(Qnet(torch.from_numpy(state).to(device).unsqueeze(0).float())).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.9999  # Discount factor\n",
    "epsilon = 1     # Initial epsilon\n",
    "decay = 100     # Epsilon decay factor\n",
    "step_epsilon = 0.001\n",
    "cont_epsilon = 0\n",
    "tau = 0.05      # Target network update factor\n",
    "batch_size = 128\n",
    "start_train = 100  # Start training after this many episodes\n",
    "max_steps = 6000\n",
    "episodes = 7000\n",
    "state_length = 9  # Number of state variables from environment\n",
    "rp_len = 10000\n",
    "\n",
    "# Initialize replay memory, agent, and networks\n",
    "rp = ReplayMemory(rp_len)\n",
    "agent = WrapAgent(epsilon, decay)\n",
    "Qnet = Red(state_length, 30, 4).to(device)\n",
    "target = Red(state_length, 30, 4).to(device)\n",
    "target.load_state_dict(Qnet.state_dict())\n",
    "optimizer = torch.optim.SGD(Qnet.parameters(), lr=3e-4)\n",
    "loss = nn.MSELoss()\n",
    "rng = np.random.default_rng(33234)\n",
    "env = rck.Rocket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Train the network by sampling transitions from the replay memory and updating Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "rew = []\n",
    "t_steps = []\n",
    "error_t = []\n",
    "\n",
    "for i in range(episodes):\n",
    "    with torch.inference_mode():\n",
    "        reward = 0\n",
    "        time_steps = 0\n",
    "        state = env.reset()\n",
    "        for j in range(max_steps):\n",
    "            action = agent.choose_action(env, state, i, Qnet, epsilon, step_epsilon, cont_epsilon)\n",
    "            new = env.step(action)\n",
    "            state_n = new[:state_length]\n",
    "            r = new[state_length]\n",
    "            done = new[-1]\n",
    "            reward += r\n",
    "            time_steps += 1\n",
    "            rp.append(np.hstack((state, r, np.array(action), state_n, done)))\n",
    "            if done == 0:\n",
    "                break\n",
    "            else:\n",
    "                state = state_n\n",
    "        rew.append(reward)\n",
    "        t_steps.append(time_steps)\n",
    "        error = 0\n",
    "        if i >= start_train:\n",
    "            for batch in rp.minibatch(batch_size):\n",
    "                Qnet.train()\n",
    "                state_t = torch.from_numpy(batch[:, :state_length]).float().to(device)\n",
    "                finish_states = torch.from_numpy(batch[:, -1]).float().unsqueeze(0).to(device)\n",
    "                sample_space = np.arange(4)\n",
    "                actions = torch.vstack(tuple(torch.from_numpy(sample_space == a) for a in batch[:, state_length+1])).to(device)\n",
    "                q_values_qnet = Qnet(state_t)[actions].unsqueeze(0)\n",
    "                state_n = torch.from_numpy(batch[:, state_length+2:-1]).float().to(device)\n",
    "                reward_b = torch.from_numpy(batch[:, state_length+1]).float().unsqueeze(0).to(device)\n",
    "                q_values_target = reward_b + gamma * torch.max(target(state_n), dim=1)[0] * finish_states\n",
    "                err_train = loss(q_values_target, q_values_qnet)\n",
    "                error += err_train.detach().cpu().numpy()\n",
    "                optimizer.zero_grad()\n",
    "                err_train.backward()\n",
    "                optimizer.step()\n",
    "                Qnet.eval()\n",
    "            error_t.append(error)\n",
    "            print(f\"Process {i*100/episodes:.2f}%, Episode {i}, Steps {t_steps[-1]}, Reward {rew[-1]}, Error {error_t[-1]:.5f}, Epsilon {max(epsilon - step_epsilon*cont_epsilon,0.01):.3f}\")\n",
    "            # Update target network\n",
    "            Qnet_dict = Qnet.state_dict()\n",
    "            target_dict = target.state_dict()\n",
    "            for key in target_dict.keys():\n",
    "                target_dict[key] = tau * Qnet_dict[key] + (1 - tau) * target_dict[key]\n",
    "            target.load_state_dict(target_dict)\n",
    "            torch.save(Qnet.state_dict(), '../models/net.pth')\n",
    "        cont_epsilon += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "torch.save(Qnet.state_dict(), '../models/net.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(np.arange(len(error_t)), error_t)\n",
    "plt.title('Training Error per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Error')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(t_steps, bins=100)\n",
    "plt.title('Distribution of Steps per Episode')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rocket Animation\n",
    "\n",
    "Visualize the rocket trajectory using `matplotlib.animation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.patches import Rectangle\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.ioff()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "state = env.reset()\n",
    "flag = True\n",
    "\n",
    "def animate(t):\n",
    "    global state\n",
    "    global flag\n",
    "    ax.clear()\n",
    "    xlim = 10\n",
    "    ylim = 10\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Select action using the trained network\n",
    "    action = torch.argmax(Qnet(torch.from_numpy(state[:state_length]).unsqueeze(0).float().to(device))).detach().cpu().item()\n",
    "    new = env.step(action)\n",
    "    state = new[:state_length]\n",
    "    phi = state[2]\n",
    "    x_cm = state[0]\n",
    "    y_cm = state[1]\n",
    "    \n",
    "    ax.set_ylim(-ylim, ylim)\n",
    "    ax.set_xlim(-xlim, xlim)\n",
    "    \n",
    "    # Draw the rocket as a rectangle\n",
    "    rect = Rectangle((x_cm-0.25, y_cm-2), width=0.5, height=4, angle=phi*180/np.pi, rotation_point='center', edgecolor='blue', facecolor='lightblue')\n",
    "    ax.add_patch(rect)\n",
    "    if flag:\n",
    "        plt.plot(np.array([-y_cm, y_cm]), np.array([x_cm, x_cm]), color='red')  # Example flame visualization\n",
    "\n",
    "FuncAnimation(fig, animate, frames=100, interval=40)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

